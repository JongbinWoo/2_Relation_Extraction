base:
  model_arc: "Bert"
  model_name: "bert-base-multilingual-cased"
  seed: 42
  train_args:
    num_workers: 5
    num_epochs: 15
    batch_size: 16
    eval_batch_size: 32
    lr: 0.00005
    weight_decay: 0.01
    warmup_steps: 500
    output_dir: './results/base'
    save_steps: 500
    save_total_limit: 3
    logging_steps: 100
    logging_dir: './logs/base'
    evaluation_strategy: 'epoch'
    eval_steps: 500
    save_model: False
  val_args:
    use_kfold: True
    num_k: 5
    test_size: 0.2
  test_args:
    '1': 2000
    '2': 2000
    '3': 2000
    '4': 2000
    '5': 2000

electra-base-v3:
  model_arc: "Electra"
  model_name: "monologg/koelectra-base-v3-discriminator"
  seed: 42
  train_args:
    num_workers: 5
    num_epochs: 15
    train_batch_size: 16
    eval_batch_size: 32
    lr: 0.00005
    weight_decay: 0.01
    warmup_steps: 500
    output_dir: './results/electra-base-v3'
    save_steps: 500
  val_args:
    use_kfold: True
    num_k: 5
    test_size: 0

xlm-roberta-base:
  model_arc: "roberta"
  model_name: "xlm-roberta-large"
  seed: 42
  train_args:
    num_workers: 5
    num_epochs: 15
    train_batch_size: 16
    eval_batch_size: 32
    lr: 0.00005
    weight_decay: 0.01
    warmup_steps: 500
    output_dir: './results/xlm-roberta-large'
    save_steps: 500
  val_args:
    use_kfold: True
    num_k: 5
    test_size: 0

